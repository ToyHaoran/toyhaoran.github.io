---
title: 窗口函数的使用
date: 2018-11-30 21:26:37
tags: spark
categories: spark
---


## 参考：

https://n3xtchen.github.io/n3xtchen/spark/2017/01/24/spark200-window-function
https://www.jianshu.com/p/42be8650509f



## 代码实现

``` scala
 object DataFrameDemo extends App {

    //1.6后基本上不用sc了，用spark.sparkContext代替
    val sc = ConnectUtil.sc
    val spark = ConnectUtil.spark
    import spark.implicits._       

     /*
        有些时候需要计算一些排序特征，窗口特征等，如一个店铺的首单特征。
        对于这样的特征显然是不能简单通过groupBy操作来完成
        即：第一列是订单，第二列是店铺，第三列是支付时间，第四列是价格。
        1、统计每个店铺每个订单和前一单的价格和，如果通过groupBy来完成特别费劲。
        2、店铺这个订单与前一单的差值，需要自定义聚合函数
        还有计算前4秒的平均值、计算环比之类的，都要用到窗口函数。
         */
     val orders = Seq(
         ("o1", "s1", "2017-05-01", 100),
         ("o2", "s1", "2017-05-02", 200),
         ("o3", "s2", "2017-05-01", 200),
         ("o4", "s1", "2017-05-03", 200),
         ("o5", "s2", "2017-05-02", 100),
         ("o6", "s1", "2017-05-04", 300)
     ).toDF("order_id", "seller_id", "pay_time", "price")
     //打印分区信息
     orders.printLocation()

     //店铺订单顺序
     val rankSpec = Window.partitionBy("seller_id").orderBy("pay_time")
     orders.withColumn("rank", dense_rank.over(rankSpec)).show()
     val rankSpec2 = Window.partitionBy("seller_id").orderBy("price")
     orders.withColumn("rank2", rank.over(rankSpec2)).show() //1,2,2,4
     orders.withColumn("dense_rank2", dense_rank.over(rankSpec2)).show()//1,2,2,3

     //定义前一单和本单的窗口
     val winSpec = Window.partitionBy("seller_id").orderBy("pay_time").rowsBetween(-1, 0)
     //店铺这个订单及前一单的价格和
     orders.withColumn("sum_pay", sum("price").over(winSpec)).show()
     //店铺这个订单与前一单的平均值，用UDAF
     def getAvgUdaf:UserDefinedAggregateFunction = new MyAverage
     orders.withColumn("avg",getAvgUdaf($"price").over(winSpec)).show()
     orders.withColumn("avg2", avg("price").over(winSpec)).show()

     //每个店铺当前订单与前一单的差值,需要自定义聚合函数，或者lag函数
     def getMinusUdaf:UserDefinedAggregateFunction = new MyMinus
     orders.withColumn("rank", dense_rank.over(rankSpec))
     .withColumn("prePrice", lag("price", 1).over(rankSpec))  //前一行的值
     .withColumn("minus", getMinusUdaf($"price").over(winSpec))  //在前面的基础上用UDF也行
     .show()

     /*
        lag(field, n): 就是取从当前字段往前第n个值，这里是取前一行的值
        first/last(): 提取这个分组特定排序的第一个最后一个，在获取用户退出的时候，你可能会用到
        lag/lead(field, n): lead 就是 lag 相反的操作，这个用于做数据回测特别用，结果回推条件
         */
     //——————————————————————以下为自定义聚合函数UDAF——————————————————————————————————————————————
     class MyAverage extends UserDefinedAggregateFunction{
         //继承抽象函数必须实现以下方法
         // 输入参数的数据类型
         def inputSchema: StructType = StructType(StructField("value", LongType) :: Nil)
         // 缓冲区中进行聚合时，所处理的数据的类型
         def bufferSchema: StructType = StructType(StructField("count", LongType) :: StructField("sum", DoubleType) :: Nil)

         // 初始化给定的聚合缓冲区，即聚合缓冲区的零值。   请注意，缓冲区内的数组和映射仍然是不可变的。
         def initialize(buffer: MutableAggregationBuffer): Unit = {
             buffer(0) = 0L  //表示次数
             buffer(1) = 0.0D  //表示总和
         }
         //使用来自input的新输入数据更新给定的聚合缓冲区`buffer`。每个输入行调用一次。
         def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
             if (!input.isNullAt(0)) {
                 buffer(0) = buffer.getLong(0) + 1L  //次数加1
                 buffer(1) = buffer.getDouble(1) + input.getAs[Long](0).toDouble  //求和
             }
         }
         // 此函数是否始终在相同输入上返回相同的输出
         def deterministic: Boolean = true
         // 合并两个聚合缓冲区并将更新的缓冲区值存储回“buffer1”。
         // 当我们将两个部分聚合的数据合并在一起时调用此方法。
         // Spark是分布式的，所以不同的区需要进行合并。
         def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
             buffer1(0) = buffer1.getLong(0) + buffer2.getLong(0)  //求次数
             buffer1(1) = buffer1.getDouble(1) + buffer2.getDouble(1)  //求和
         }
         //计算最终的结果
         def evaluate(buffer: Row): Double = {
             buffer.getDouble(1) / buffer.getLong(0).toDouble
         }
         // 返回值的数据类型
         def dataType: DataType = DoubleType
     }

    class MyMinus extends UserDefinedAggregateFunction{
        def inputSchema: StructType = StructType(StructField("value", LongType) :: Nil)
        def bufferSchema: StructType = StructType(StructField("minus", LongType) :: Nil)
        def initialize(buffer: MutableAggregationBuffer): Unit = {
            buffer(0) = 0L  //表示差值
        }
        def update(buffer: MutableAggregationBuffer, input: Row): Unit = {
            if (!input.isNullAt(0)) {
                //输入的后者减去前者
                buffer(0) = input.getLong(0) - buffer.getLong(0)
            }
        }
        // 此函数是否始终在相同输入上返回相同的输出
        def deterministic: Boolean = true
        def merge(buffer1: MutableAggregationBuffer, buffer2: Row): Unit = {
            //分区合并，也是后者减前者
            buffer1(0) = buffer2.getLong(0) - buffer1.getLong(0)
        }
        //计算最终的结果
        def evaluate(buffer: Row): Long = {
            buffer.getLong(0)
        }
        // 返回值的数据类型
        def dataType: DataType = LongType
    }
     
 }
     
```

